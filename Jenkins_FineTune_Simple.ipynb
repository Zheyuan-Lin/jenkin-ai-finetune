{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤– Simple Jenkins Chatbot Fine-Tuning\n",
        "\n",
        "**Minimal fine-tuning script for Llama-2 on Jenkins Q&A data**\n",
        "\n",
        "### Requirements:\n",
        "- Google Colab with GPU runtime (T4 or better)\n",
        "- Google Drive for saving model\n",
        "- ~10GB GPU VRAM\n",
        "- ~2 hours training time\n",
        "\n",
        "### What this does:\n",
        "1. Installs required packages\n",
        "2. Loads Jenkins Q&A data\n",
        "3. Fine-tunes Llama-2-7b with QLoRA\n",
        "4. Saves model to Google Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets\n",
        "\n",
        "print(\"âœ“ Packages installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Mount Google Drive (to save model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ“ Google Drive mounted!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Clone Repository and Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository with training data\n",
        "!git clone https://github.com/YOUR_USERNAME/Enhancing-LLM-with-Jenkins-Knowledge.git\n",
        "%cd Enhancing-LLM-with-Jenkins-Knowledge\n",
        "\n",
        "print(\"âœ“ Repository cloned!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def format_instruction(question, answer):\n",
        "    \"\"\"Format Q&A in Llama-2 instruction format.\"\"\"\n",
        "    return f\"<s>[INST] {question.strip()} [/INST] {answer.strip()} </s>\"\n",
        "\n",
        "# Load Jenkins datasets\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Dataset 1: Stack Overflow\n",
        "df1 = pd.read_csv('datasets/QueryResultsUpdated.csv')\n",
        "df1['text'] = df1.apply(lambda x: format_instruction(x['Question Body'], x['Answer Body']), axis=1)\n",
        "\n",
        "# Dataset 2: Jenkins Docs\n",
        "df2 = pd.read_csv('datasets/Jenkins Docs QA.csv')\n",
        "df2['text'] = df2.apply(lambda x: format_instruction(x['Question'], x['Answer']), axis=1)\n",
        "\n",
        "# Dataset 3: Community Questions\n",
        "df3 = pd.read_csv('datasets/Community Questions Refined.csv')\n",
        "df3['text'] = df3.apply(lambda x: format_instruction(x['questions'], x['answers']), axis=1)\n",
        "\n",
        "# Combine all datasets\n",
        "all_data = pd.concat([df1[['text']], df2[['text']], df3[['text']]], ignore_index=True)\n",
        "\n",
        "# Remove very long examples (keep under 2000 chars)\n",
        "all_data = all_data[all_data['text'].str.len() < 2000]\n",
        "\n",
        "# Remove empty entries\n",
        "all_data = all_data.dropna()\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_pandas(all_data)\n",
        "\n",
        "print(f\"\\nâœ“ Loaded {len(dataset)} training examples\")\n",
        "print(f\"\\nSample example:\")\n",
        "print(dataset[0]['text'][:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load Model with 4-bit Quantization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Model to fine-tune\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# 4-bit quantization config (saves memory)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"âœ“ Model and tokenizer loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Setup LoRA (Parameter-Efficient Fine-Tuning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                      # LoRA rank\n",
        "    lora_alpha=16,             # LoRA alpha\n",
        "    lora_dropout=0.05,         # Dropout\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to adapt\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Show trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n",
        "print(f\"Percentage: {100 * trainable_params / total_params:.2f}%\")\n",
        "print(\"\\nâœ“ LoRA adapters added!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Where to save the model\n",
        "output_dir = \"/content/drive/MyDrive/jenkins-llama-model\"\n",
        "\n",
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,                    # Number of epochs (increase for better results)\n",
        "    per_device_train_batch_size=4,         # Batch size\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-4,                    # Learning rate\n",
        "    logging_steps=25,                      # Log every N steps\n",
        "    save_steps=100,                        # Save checkpoint every N steps\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    fp16=True,                             # Use mixed precision\n",
        "    report_to=\"none\",                      # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,                    # Max sequence length\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Total examples: {len(dataset)}\")\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"This will take ~1-2 hours on T4 GPU\\n\")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nâœ“ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save the Fine-Tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and tokenizer\n",
        "final_model_path = \"/content/drive/MyDrive/jenkins-llama-final\"\n",
        "\n",
        "print(f\"Saving model to: {final_model_path}\")\n",
        "trainer.model.save_pretrained(final_model_path)\n",
        "trainer.tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "print(\"\\nâœ“ Model saved successfully!\")\n",
        "print(f\"\\nYour fine-tuned model is saved at: {final_model_path}\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Test the model\")\n",
        "print(\"2. Merge LoRA weights with base model\")\n",
        "print(\"3. Convert to GGUF format for deployment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test the Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test\n",
        "def test_model(question):\n",
        "    \"\"\"Test the fine-tuned model with a question.\"\"\"\n",
        "    prompt = f\"<s>[INST] {question} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the answer part\n",
        "    answer = response.split(\"[/INST]\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# Test with some questions\n",
        "test_questions = [\n",
        "    \"What is Jenkins?\",\n",
        "    \"How do I install Jenkins?\",\n",
        "    \"What is a Jenkins pipeline?\",\n",
        "]\n",
        "\n",
        "print(\"Testing the model:\\n\")\n",
        "for question in test_questions:\n",
        "    print(f\"Q: {question}\")\n",
        "    answer = test_model(question)\n",
        "    print(f\"A: {answer}\\n\")\n",
        "    print(\"-\" * 80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ‰ Done!\n",
        "\n",
        "Your Jenkins-specialized Llama-2 model is now fine-tuned!\n",
        "\n",
        "### What you have:\n",
        "- âœ… Fine-tuned LoRA weights saved to Google Drive\n",
        "- âœ… Model specialized for Jenkins questions\n",
        "- âœ… ~300MB adapter weights (not full 13GB model)\n",
        "\n",
        "### To use in production:\n",
        "1. **Merge LoRA weights** with base model to create standalone model\n",
        "2. **Quantize to GGUF** format for CPU inference (6-7GB)\n",
        "3. **Deploy** with Flask backend from this repo\n",
        "\n",
        "### Resources:\n",
        "- [Full Documentation](https://github.com/YOUR_USERNAME/Enhancing-LLM-with-Jenkins-Knowledge)\n",
        "- [Convert to GGUF Guide](https://github.com/ggerganov/llama.cpp)\n",
        "- [HuggingFace Model Upload](https://huggingface.co/docs/hub/models-uploading)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
